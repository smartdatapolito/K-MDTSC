{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from kshape.core import kshape, zscore\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeSSE(dataset,labels,centroids):\n",
    "    \n",
    "    SSE = 0\n",
    "    for i in set(labels):\n",
    "        for j in range(len(labels)):\n",
    "            if(i==labels[j]):\n",
    "                SSE += calc_distance(dataset[j], centroids[i])**2\n",
    "    \n",
    "    return SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MDTSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(s1, s2):\n",
    "    \n",
    "    if(len(s1)==0 or len(s2)==0): return 0\n",
    "    \n",
    "    d = 0\n",
    "    \n",
    "    for dimension in range(len(s1)):\n",
    "\n",
    "        for i in range(len(s1[dimension])):\n",
    "            d += ((s1[dimension][i]-s2[dimension][i])**2)\n",
    "\n",
    "    d=d**0.5\n",
    "\n",
    "    return d\n",
    "\n",
    "def assign_to_clusters(c,s1):\n",
    "    \n",
    "    label = -1\n",
    "    min_dist = 100000000000\n",
    "    \n",
    "    for i in range(len(c)):\n",
    "        ci = c[i]\n",
    "        tmp_dist = calc_distance(s1, ci)\n",
    "        if(tmp_dist < min_dist): \n",
    "            min_dist = tmp_dist\n",
    "            label=i\n",
    "    \n",
    "    return label\n",
    "\n",
    "def compute_centroid(points):\n",
    "   \n",
    "    if(len(points)==0): return []\n",
    "    \n",
    "    centroid = [[0 for i in range(len(points[0][metrics]))] for metrics in range(len(points[0]))]\n",
    "    \n",
    "    for point in points:\n",
    "        for metric in range(len(point)):\n",
    "            samples = point[metric]\n",
    "            for i in range(len(samples)):\n",
    "                centroid[metric][i]+=samples[i]\n",
    "    \n",
    "    for metric in range(len(centroid)):\n",
    "        for i in range(len(centroid[metric])):\n",
    "            centroid[metric][i]/=len(points)\n",
    "    \n",
    "    \n",
    "    return centroid\n",
    "\n",
    "\n",
    "def assign_one_point(cluster_points,empty_cluster):\n",
    "    \n",
    "    most_popular = 0\n",
    "    for cluster in cluster_points:\n",
    "        if(len(cluster_points[cluster])>len(cluster_points[most_popular])):\n",
    "            most_popular = cluster\n",
    "            \n",
    "    farest = -1\n",
    "    farest_point = 0\n",
    "    ci = compute_centroid(cluster_points[most_popular])\n",
    "    for i in range(len(cluster_points[most_popular])):\n",
    "        s1 = cluster_points[most_popular][i]\n",
    "        tmp_dist = calc_distance(s1, ci)\n",
    "        if(tmp_dist>farest):\n",
    "            farest=tmp_dist\n",
    "            farest_point = i\n",
    "    \n",
    "    \n",
    "    s1 = cluster_points[most_popular][farest_point]\n",
    "    cluster_points[empty_cluster].append(s1)\n",
    "    \n",
    "    del cluster_points[most_popular][farest_point]\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def create_clusters(dataset, nclusters):\n",
    "\n",
    "    current_centroids = random.sample(dataset, nclusters)\n",
    "    current_distance = -1 \n",
    "\n",
    "    iterations = 100\n",
    "    labels = []\n",
    "    iteration=0\n",
    "    for iteration in range(iterations):\n",
    "\n",
    "        cluster_points = {cluster:[] for cluster in range(nclusters)}\n",
    "        labels = []        \n",
    "        for s1 in dataset:\n",
    "            cluster = assign_to_clusters(current_centroids,s1)\n",
    "            cluster_points[cluster].append(s1)    \n",
    "            labels.append(cluster)\n",
    "\n",
    "        for i in range(nclusters):\n",
    "            if(len(cluster_points[i])==0):\n",
    "                assign_one_point(cluster_points,i)\n",
    "\n",
    "        new_centroids = []\n",
    "        dist = 0 \n",
    "        for i in range(nclusters):\n",
    "            centroidi = compute_centroid(cluster_points[i])\n",
    "            new_centroids.append(centroidi)\n",
    "            \n",
    "            dist +=calc_distance(current_centroids[i], new_centroids[i])\n",
    "\n",
    "        if(dist==0): \n",
    "            return labels,new_centroids,cluster_points\n",
    "\n",
    "        current_distance=dist\n",
    "        current_centroids = new_centroids\n",
    "    \n",
    "    return labels, current_centroids, cluster_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cluster(dataset,families,attempts,minc,maxc):\n",
    "        \n",
    "    Results = {}\n",
    "    for nclusters in range(minc,maxc):\n",
    "        Results[nclusters] = []\n",
    "        for attempt in range(attempts):\n",
    "            labels,centroids, cluster_points = create_clusters(dataset,nclusters)\n",
    "            \n",
    "            SSE = ComputeSSE(dataset,labels,centroids)\n",
    "            Rand = adjusted_rand_score(labels, [i%families for i in range(len(labels))])\n",
    "            \n",
    "            Results[nclusters].append({\"labels\": labels.copy(),\"centroids\":centroids.copy(),\"SSE\":SSE,\"RandIndex\":Rand})\n",
    "\n",
    "    return Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_centroids(dataset,clusters):\n",
    "\n",
    "    denorm_centroids = {i:[] for i in range(len(clusters))}\n",
    "\n",
    "    nc = 0\n",
    "    for cluster in clusters:\n",
    "        centroid = cluster[0]\n",
    "        components = {}\n",
    "        for dimension in range(len(dataset[0])):\n",
    "            components[dimension] = [[] for _ in range(len(dataset[0][dimension]))]\n",
    "            \n",
    "        for pointid in cluster[1]:\n",
    "            point = dataset[pointid]\n",
    "            for dimension in range(len(point)):\n",
    "                samples = point[dimension]\n",
    "                for i in range(len(samples)):\n",
    "                    components[dimension][i].append(samples[i])\n",
    "\n",
    "        for dimension in range(len(dataset[0])):     \n",
    "            nsamples = len(dataset[0][dimension])\n",
    "            centroid_dimnension = centroid[dimension*nsamples:(dimension+1)*nsamples].copy()\n",
    "            for k in range(len(components[dimension])):\n",
    "                centroid_dimnension[k] = centroid_dimnension[k]*np.std(components[dimension][k])+np.mean(components[dimension][k])\n",
    "        \n",
    "            denorm_centroids[nc].append(centroid_dimnension) \n",
    "        nc+=1\n",
    "    return denorm_centroids\n",
    "\n",
    "def get_labels(dataset_kshape,clusters):\n",
    "    \n",
    "    labels = [0 for _ in range(len(dataset_kshape))]\n",
    "    nc=0\n",
    "    for cluster in clusters:\n",
    "        for pointid in cluster[1]: labels[pointid]=nc\n",
    "        nc+=1\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_shape(dataset,families,attempts,minc,maxc):\n",
    "\n",
    "    dataset_kshape = []\n",
    "    for p in dataset:\n",
    "        pg = []\n",
    "        for i in range(len(p)):\n",
    "            pg =pg + list(p[i]) \n",
    "        dataset_kshape.append(pg)    \n",
    "        \n",
    "    Results = {}\n",
    "    \n",
    "    for nclusters in range(minc,maxc):\n",
    "        Results[nclusters]=[]\n",
    "        for attempt in range(attempts):\n",
    "            clusters = kshape(dataset_kshape, nclusters)\n",
    "            centroids = denormalize_centroids(dataset,clusters)\n",
    "            labels = get_labels(dataset_kshape,clusters)\n",
    "\n",
    "            SSE = ComputeSSE(dataset,labels,centroids)\n",
    "            Rand = adjusted_rand_score(labels, [i%families for i in range(len(labels))])\n",
    "\n",
    "            Results[nclusters].append({\"labels\": labels.copy(),\"centroids\":centroids.copy(),\"SSE\":SSE,\"RandIndex\":Rand})\n",
    "                           \n",
    "    return Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SinNoise(x,period=1,noise=0.1):\n",
    "    return np.sin(x*period) + np.random.normal(scale=noise, size=len(x))\n",
    "\n",
    "def CosNoise(x,period=1,noise=0.1):\n",
    "    return np.cos(x*period) + np.random.normal(scale=noise, size=len(x))\n",
    "\n",
    "def generate_points(npoints, families=4, dimensions=2, noise=0.1):\n",
    "    \n",
    "    points = []\n",
    "\n",
    "    family_config_periods_dimension = {}\n",
    "    for i in range(families):\n",
    "        periods_dimension = [1+int(i/4),1+int(i/4)]+\\\n",
    "        [random.uniform(2+int(i/4),10+int(i/4)) for _ in range(dimensions-2)]\n",
    "        \n",
    "        family_config_periods_dimension[i] = periods_dimension\n",
    "    \n",
    "    for i in range(npoints):\n",
    "        dimensions_values=[]\n",
    "        family = i%families\n",
    "        config = family_config_periods_dimension[family][-1]\n",
    "        x = np.linspace(0, 2*np.pi)\n",
    "        if(config==0):\n",
    "            for j in range(dimensions):\n",
    "                if(j%2==0):\n",
    "                    si1 = SinNoise(x,period=family_config_periods_dimension[family][j],noise=noise)\n",
    "                    dimensions_values.append(si1)\n",
    "                else:\n",
    "                    si2 = CosNoise(x,period=family_config_periods_dimension[family][j],noise=noise)\n",
    "                    dimensions_values.append(si2)\n",
    "        if(config==1):\n",
    "            for j in range(dimensions):\n",
    "                if(j%2==0):\n",
    "                    si1 = CosNoise(x,noise=noise,period=family_config_periods_dimension[family][j])\n",
    "                    dimensions_values.append(si1)\n",
    "                else:\n",
    "                    si2 = SinNoise(x,noise=noise,period=family_config_periods_dimension[family][j])\n",
    "                    dimensions_values.append(si2)\n",
    "        if(config==2):\n",
    "            for j in range(dimensions):\n",
    "                if(j%2==0):\n",
    "                    si1 = SinNoise(x,noise=noise,period=family_config_periods_dimension[family][j])\n",
    "                    dimensions_values.append(si1)\n",
    "                else:\n",
    "                    si2 = SinNoise(x,noise=noise,period=family_config_periods_dimension[family][j]) \n",
    "                    dimensions_values.append(si2)\n",
    "        if(config==3):\n",
    "            for j in range(dimensions):\n",
    "                if(j%2==0):\n",
    "                    si1 = CosNoise(x,noise=noise,period=family_config_periods_dimension[family][j])\n",
    "                    dimensions_values.append(si1)\n",
    "                else:\n",
    "                    si2 = CosNoise(x,noise=noise,period=family_config_periods_dimension[family][j]) \n",
    "                    dimensions_values.append(si2)\n",
    "        \n",
    "        points.append(dimensions_values)\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Synthetic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of multi-dimensional time-series \n",
    "Points = 200\n",
    "#Numer of runs for each clustering algorithm\n",
    "attempts = 10\n",
    "\n",
    "#Load the Dataset used to run the paper experiment \n",
    "dataset = pickle.load(open(\"result/Dataset_Synthetic.pkl\",\"rb\"))\n",
    "#Decomment to create a new random dataset\n",
    "#dataset = generate_points(Points, families=4, dimensions=2, noise=0.1)\n",
    "\n",
    "ResultKMDTSC = run_cluster(dataset,families=4, attempts=attempts, minc=1, maxc=11)\n",
    "ResultkShape = run_k_shape(dataset,families=4, attempts=attempts, minc=1, maxc=11)\n",
    "\n",
    "pickle.dump(dataset, open(\"result/Dataset_Synthetic.pkl\",\"wb\"))\n",
    "pickle.dump(ResultKMDTSC, open(\"result/KMDTSC_Synthetic.pkl\",\"wb\")) \n",
    "pickle.dump(ResultkShape, open(\"result/kShape_Synthetic.pkl\",\"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perturbation stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsKMDTSC = {}\n",
    "ResultskShape = {}\n",
    "\n",
    "Datasets = {}\n",
    "#Load the Datasets used to run the paper experiment \n",
    "Datasets = pickle.load(open(\"result/Datasets_Perturbation.pkl\",\"rb\"))\n",
    "\n",
    "#Number of multi-dimensional time-series \n",
    "Points = 200\n",
    "#Numer of runs for each clustering algorithm\n",
    "attempts = 10\n",
    "\n",
    "for i in range(3,5):\n",
    "    for noisei in np.arange(0,2.1,0.1).round(decimals=1):\n",
    "        \n",
    "        dataset = Datasets[noisei]\n",
    "        #Decomment to create a new random dataset\n",
    "        #dataset = generate_points(Points, families=4, dimensions=2, noise=noisei)\n",
    "\n",
    "        ResultKMDTSC = run_cluster(dataset,families=4, attempts=attempts, minc=4, maxc=5)\n",
    "        ResultkShape = run_k_shape(dataset,families=4, attempts=attempts, minc=4, maxc=5)\n",
    "\n",
    "        #Datasets[noisei] = dataset\n",
    "        ResultsKMDTSC[noisei] = ResultKMDTSC\n",
    "        ResultskShape[noisei] = ResultkShape\n",
    "\n",
    "    pickle.dump(Datasets, open(\"result/Datasets_Perturbation.pkl\",\"wb\"))\n",
    "    pickle.dump(ResultsKMDTSC, open(\"result/KMDTSC_Perturbation.pkl\",\"wb\")) \n",
    "    pickle.dump(ResultskShape, open(\"result/kShape_Perturbation.pkl\",\"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Family stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsKMDTSC = {}\n",
    "ResultskShape = {}\n",
    "\n",
    "Datasets = {}\n",
    "#Load the Datasets used to run the paper experiment \n",
    "Datasets = pickle.load(open(\"result/Datasets_Family.pkl\",\"rb\"))\n",
    "\n",
    "#Number of multi-dimensional time-series \n",
    "Points = 200\n",
    "#Numer of runs for each clustering algorithm\n",
    "attempts = 10\n",
    "for i in range(4):\n",
    "\n",
    "    for familyi in range(2,11):\n",
    "        dataset = Datasets[familyi]\n",
    "        #Decomment to create a new random dataset\n",
    "        #dataset = generate_points(Points, families=familyi, dimensions=2, noise=0.1)\n",
    "\n",
    "        ResultKMDTSC = run_cluster(dataset, families=familyi, attempts=attempts, minc=familyi, maxc=familyi+1)\n",
    "        ResultkShape = run_k_shape(dataset, families=familyi, attempts=attempts, minc=familyi, maxc=familyi+1)\n",
    "\n",
    "        Datasets[familyi] = dataset\n",
    "        ResultsKMDTSC[familyi] = ResultKMDTSC\n",
    "        ResultskShape[familyi] = ResultkShape\n",
    "\n",
    "    pickle.dump(Datasets, open(\"result/Datasets_Family.pkl\",\"wb\"))\n",
    "    pickle.dump(ResultsKMDTSC, open(\"result/KMDTSC_Family.pkl\",\"wb\")) \n",
    "    pickle.dump(ResultskShape, open(\"result/kShape_Family.pkl\",\"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResultsKMDTSC = {}\n",
    "ResultskShape = {}\n",
    "\n",
    "Datasets = {}\n",
    "#Load the Datasets used to run the paper experiment \n",
    "Datasets = pickle.load(open(\"result/Datasets_Dimension.pkl\",\"rb\"))\n",
    "\n",
    "#Number of multi-dimensional time-series \n",
    "Points = 200\n",
    "#Numer of runs for each clustering algorithm\n",
    "attempts = 10\n",
    "noisei = 0.1\n",
    "for dimensions in range(2,11):\n",
    "\n",
    "    dataset = Datasets[dimensions]\n",
    "    #Decomment to create a new random dataset\n",
    "    #dataset = generate_points(Points, families=4, dimensions=dimensions, noise=0.1)\n",
    "\n",
    "    ResultKMDTSC = run_cluster(dataset, families=4, attempts=attempts, minc=4, maxc=5)\n",
    "    ResultkShape = run_k_shape(dataset, families=4, attempts=attempts, minc=4, maxc=5)\n",
    "\n",
    "    Datasets[dimensions] = dataset\n",
    "    ResultsKMDTSC[dimensions] = ResultKMDTSC\n",
    "    ResultskShape[dimensions] = ResultkShape\n",
    "\n",
    "pickle.dump(Datasets, open(\"result/Datasets_Dimension.pkl\",\"wb\"))\n",
    "pickle.dump(ResultsKMDTSC, open(\"result/KMDTSC_Dimension.pkl\",\"wb\")) \n",
    "pickle.dump(ResultskShape, open(\"result/kShape_Dimension.pkl\",\"wb\")) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
